{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(url):\n",
    "    try:\n",
    "        f1 = len(url)\n",
    "\n",
    "        symbols = 0\n",
    "        alpha_num = 0\n",
    "        for i in range(len(url)):\n",
    "            if(url[i].isalnum()):\n",
    "                alpha_num += 1\n",
    "            else: \n",
    "                symbols += 1\n",
    "\n",
    "        f2 = symbols/alpha_num\n",
    "\n",
    "        sus_symbol_count = 0\n",
    "        sus_symbols = ['`','%','#','^','$','&','-','*',':']\n",
    "        for i in range(len(url)):\n",
    "            if(url[i] in sus_symbols):\n",
    "                sus_symbol_count += 1\n",
    "\n",
    "        f3 = sus_symbol_count\n",
    "\n",
    "        f4 = len(urlparse(url).path)\n",
    "\n",
    "        sus_keywords = [ 'submit', 'secure', 'suspend','confirm', 'webscr',\n",
    "                        'account','login', 'signin', 'logon', \n",
    "                        'cmd', 'update', 'wp', 'index', 'payment',\n",
    "                        'home', 'paypal', 'webhostapp', 'dropbox']\n",
    "\n",
    "        all_str = ''\n",
    "        for s in range(len(url)):\n",
    "            if url[s].isalpha():\n",
    "                all_str += url[s]\n",
    "\n",
    "        sus_keyword_count = 0\n",
    "        for i in range(len(url)):\n",
    "            for j in range(i+1, len(url)+1):\n",
    "                sub_str= url[i: j]\n",
    "                if sub_str in sus_keywords:\n",
    "                    sus_keyword_count += 1\n",
    "\n",
    "        f5 = sus_keyword_count\n",
    "\n",
    "        protocol_used = urlparse(url).scheme # finds https/http/ftp etc.\n",
    "        f6 = 0 if(protocol_used == '') else 1 \n",
    "\n",
    "        f7 = url.count('-')\n",
    "\n",
    "        f8 = 0 if(url[-1].isalpha()) else 1 #last character is a symbol\n",
    "\n",
    "        redirection_count = 0\n",
    "        for i in range(1, len(url)-1):\n",
    "            if(url[i] == '/'):\n",
    "                if(url[i-1] == ':' and url[i+1] == '/'):\n",
    "                    continue\n",
    "                elif(url[i+1] == '/'):\n",
    "                    redirection_count += 1\n",
    "\n",
    "        f9 = 0 if(redirection_count == 0) else 1   \n",
    "\n",
    "        f10 = 0 if(url.count('@') == 0) else 1 #presence of @\n",
    "\n",
    "        f11 = url.count('/') - 2*url.count('//') # not counting the // slashes\n",
    "\n",
    "        # regex to check if url contains IP address\n",
    "        f12 = 0 if(re.match(r'http://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}/.*', url) is None) else 1 \n",
    "\n",
    "        f13 = url.count('?')\n",
    "\n",
    "        f14 = len(tldextract.extract(url).subdomain.split('.')) # NO. of subdomains\n",
    "\n",
    "        if (url.find('www') == -1):\n",
    "            f15 = 1 # no www found means phish\n",
    "        else:\n",
    "            f15 = 0\n",
    "\n",
    "        if (url.find('http') == -1 or url.find('https') == -1):\n",
    "            f16 = 0 # not found means phish\n",
    "        else:\n",
    "            f16 = 1\n",
    "\n",
    "        f17 = 1 if(urlparse(url).port is not None) else 0\n",
    "\n",
    "        f18 = 0 if(len(url) == len(url.encode())) else 1 #If not ascii, then other unicode symbols are present\n",
    "\n",
    "        features = list([f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18])\n",
    "    #     print(len(features))\n",
    "    #     print(features)\n",
    "        return(features)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return list([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(path_to_csv):\n",
    "    data_list=[]\n",
    "\n",
    "    df_url = pd.read_csv(path_to_csv)\n",
    "    # phishing urls are only 104438 in number.\n",
    "    df_equal = df_url.groupby('label').sample(n=104438, random_state=1) # getting equal no. of both classes\n",
    "#     print(df_equal.shape)\n",
    "    \n",
    "    for index, row in tqdm(df_equal.iterrows()):\n",
    "        final_feature = get_feature(row['url'])\n",
    "        if(len(final_feature) == 0): # If there's error and the features returned are 0, then continue\n",
    "            continue\n",
    "        else:\n",
    "            class_label = row['result']\n",
    "\n",
    "            final_feature.insert(0,row['url'])\n",
    "            final_feature.insert(1,class_label)\n",
    "\n",
    "            data_list.append(final_feature)\n",
    "        \n",
    "       \n",
    "    return(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208876it [04:04, 853.16it/s] \n"
     ]
    }
   ],
   "source": [
    "csv_path = 'YOUR_PATH_TO_CSV/urldata.csv'\n",
    "\n",
    "data_list1 = feature_extraction(csv_path)\n",
    "\n",
    "df = pd.DataFrame(data_list1)\n",
    "# #  --------------------------------------------------------------------------------------\n",
    "                                \n",
    "df.rename(columns = {0: \"url\", 1: \"label\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amazon.co.uk/find-me-olwen-wymark-...</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0.237705</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.marcelmusic.com/</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.mediafire.com/?j4ymvncnobm</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://gray-seddon-tea.com/</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wn.com/Quebec_St_Malo_Race</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  label    2         3  \\\n",
       "0  https://www.amazon.co.uk/find-me-olwen-wymark-...      0  151  0.237705   \n",
       "1                       https://www.marcelmusic.com/      0   28  0.272727   \n",
       "2             https://www.mediafire.com/?j4ymvncnobm      0   38  0.225806   \n",
       "3                       https://gray-seddon-tea.com/      0   28  0.333333   \n",
       "4             https://www.wn.com/Quebec_St_Malo_Race      0   38  0.310345   \n",
       "\n",
       "    4   5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  \n",
       "0  17  29  0  1  4  1   0   0   2   0   1   1   0   1   0   0  \n",
       "1   1   1  0  1  0  1   0   0   1   0   0   1   0   1   0   0  \n",
       "2   1   1  0  1  0  0   0   0   1   0   1   1   0   1   0   0  \n",
       "3   3   1  0  1  2  1   0   0   1   0   0   1   1   1   0   0  \n",
       "4   1  20  0  1  0  0   0   0   1   0   0   1   0   1   0   0  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208875, 20)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('url_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('url_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208875, 18)\n",
      "(208875,)\n"
     ]
    }
   ],
   "source": [
    "array=df.values\n",
    "x_feature=array[:,2:]\n",
    "y_label=array[:,1].astype('int')\n",
    "print(x_feature.shape)\n",
    "print(y_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(x_feature,y_label,test_size=0.10,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data after splitting to avoid information leak between train and test set.\n",
    "\n",
    "scaler_norm = MinMaxScaler()\n",
    "\n",
    "X_train = scaler_norm.fit_transform(X_train)\n",
    "X_test = scaler_norm.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.992558 (0.000762)\n"
     ]
    }
   ],
   "source": [
    "# Random check on 10 folds cross-validation\n",
    "model_SVC=SVC(kernel='rbf',C=100,gamma=0.001)\n",
    "\n",
    "kfold=KFold(n_splits=10, shuffle=True)\n",
    "cv_results=cross_val_score(model_SVC,X_train,Y_train,cv=kfold,scoring='accuracy')\n",
    "msg=\"%s %f (%f)\" % ('Training Accuracy: ',cv_results.mean(),cv_results.std())\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9924358483339717\n",
      "[[10492    25]\n",
      " [  133 10238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     10517\n",
      "           1       1.00      0.99      0.99     10371\n",
      "\n",
      "    accuracy                           0.99     20888\n",
      "   macro avg       0.99      0.99      0.99     20888\n",
      "weighted avg       0.99      0.99      0.99     20888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_SVC = SVC(C=100,gamma=0.001, kernel='rbf')\n",
    "model_SVC.fit(X_train,Y_train) \n",
    "\n",
    "predictions=model_SVC.predict(X_test)\n",
    "\n",
    "print(accuracy_score(Y_test,predictions))\n",
    "print(confusion_matrix(Y_test,predictions))\n",
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This is an implementation of the paper -**\"Phishing URL detection system based on URL features using SVM\"** (http://eses.net.in/documents/paper5.2.3.pdf). Please cite this for any usage.\n",
    "The dataset used for this is taken from here - https://www.kaggle.com/siddharthkumar25/malicious-and-benign-urls\n",
    "\n",
    "The dataset is different from the one mentioned in the paper. However, the feature extraction process and the svm classification is the same. We have used 104,437 phishing urls and 104,438 genuine urls, in total 208,875 urls. We have extracted 18 features f1 to f18, and used all for the classification while in the paper they only proceeded with using the first 15 features.\n",
    "The slight difference in result accuracy(99.24%) is due to a much bigger, different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
